# VerbumCare Backend Configuration Example
# Copy this file to .env and update with your actual values

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
DATABASE_URL=postgres://demo:demo123@localhost:5432/verbumcare_demo

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
PORT=3000
NODE_ENV=development

# Allowed client origins (CORS)
CLIENT_URLS=http://localhost:5173,http://localhost:5174,http://localhost:19006

# ============================================================================
# AI SERVICES CONFIGURATION (Offline Mode - Two-Machine Setup)
# ============================================================================

# IMPORTANT: These services run on M2 Mac (AI machine), NOT on this server!
# Use mDNS hostnames for zero-config networking (works on LAN without internet)

# Whisper Service (Audio Transcription)
# - Runs on: M2 Mac (verbumcare-ai.local)
# - Model: Whisper Large-v3 for Japanese
# - Port: 8080
WHISPER_URL=http://verbumcare-ai.local:8080
WHISPER_MODEL=large-v3
WHISPER_LANGUAGE=ja

# Ollama Service (LLM for structured data extraction)
# - Runs on: M2 Mac (verbumcare-ai.local)
# - Model: Llama 3 8B Q4 (quantized for efficiency)
# - Port: 11434
OLLAMA_URL=http://verbumcare-ai.local:11434
OLLAMA_MODEL=llama3:8b-q4_K_M

# Ollama Performance Tuning
# - Reduced context window saves memory
# - More threads = faster processing on M2
OLLAMA_NUM_CTX=2048
OLLAMA_NUM_THREAD=8
OLLAMA_TEMPERATURE=0.1

# ============================================================================
# FALLBACK: If mDNS hostnames don't work
# ============================================================================
# Replace .local hostnames with actual IP addresses of M2 Mac
# Find M2 Mac IP: On M2 Mac, run: ifconfig | grep "inet " | grep -v 127.0.0.1
#
# WHISPER_URL=http://192.168.1.100:8080
# OLLAMA_URL=http://192.168.1.100:11434

# ============================================================================
# REMOVED: OpenAI API (No longer used - completely offline!)
# ============================================================================
# OPENAI_API_KEY=sk-...  # REMOVED - using local AI models instead

# ============================================================================
# DEPLOYMENT NOTES
# ============================================================================
#
# Intel Mac (Server - 16GB):
#   - Runs this backend + PostgreSQL + Admin Portal + Dashboard
#   - Memory usage: ~5GB
#   - Connects to M2 Mac for AI processing
#
# M2 Mac (AI - 8GB):
#   - Runs Ollama + Whisper (NO backend services)
#   - Memory usage: 3.5GB idle, 7GB peak during processing
#   - Set hostname: sudo scutil --set LocalHostName verbumcare-ai
#
# Network:
#   - Portable WiFi (LAN-only, offline)
#   - mDNS for service discovery
#   - ~2-3 seconds network overhead (negligible)
#
# Processing Time:
#   - Total: 22-30 seconds per voice recording
#   - Whisper: 8-12 seconds
#   - Ollama: 10-15 seconds
#   - Network: 2-3 seconds
#
# Startup Scripts:
#   - Intel Mac: ./intel-mac-start.sh
#   - M2 Mac: ./m2-mac-start.sh
#
# Pre-Demo Checklist:
#   - See PRE_DEMO_CHECKLIST.md for complete setup verification
#
# ============================================================================