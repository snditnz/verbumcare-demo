import axios from 'axios';

/**
 * Ollama Service Integration
 * Provides local LLM inference for medical data extraction
 * Optimized for Japanese medical terminology with reduced context
 */

const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'llama3:8b-q4_K_M';

// Reduced context window for medical extraction (saves memory)
const OLLAMA_NUM_CTX = parseInt(process.env.OLLAMA_NUM_CTX || '2048');
const OLLAMA_NUM_THREAD = parseInt(process.env.OLLAMA_NUM_THREAD || '8');
const OLLAMA_TEMPERATURE = parseFloat(process.env.OLLAMA_TEMPERATURE || '0.1');

class OllamaService {
  constructor() {
    this.isInitialized = false;
    this.isLoaded = false;
    this.modelName = OLLAMA_MODEL;
  }

  /**
   * Initialize Ollama service connection
   */
  async initialize() {
    try {
      const response = await axios.get(`${OLLAMA_URL}/api/tags`, { timeout: 5000 });
      this.isInitialized = response.status === 200;

      // Check if our model is available
      const models = response.data.models || [];
      const modelAvailable = models.some(m => m.name.includes('llama3'));

      if (!modelAvailable) {
        console.warn(`‚ö†Ô∏è  Model ${OLLAMA_MODEL} not found. Available:`, models.map(m => m.name));
      }

      console.log('‚úÖ Ollama service connected:', OLLAMA_URL);
      return this.isInitialized;
    } catch (error) {
      console.warn('‚ö†Ô∏è  Ollama service not available:', error.message);
      this.isInitialized = false;
      return false;
    }
  }

  /**
   * Pre-warm the model (loads into memory)
   */
  async prewarm() {
    try {
      console.log(`üî• Pre-warming Ollama ${OLLAMA_MODEL}...`);

      // Send a minimal request to load model into memory
      await axios.post(`${OLLAMA_URL}/api/generate`, {
        model: OLLAMA_MODEL,
        prompt: 'Hello',
        stream: false,
        options: {
          num_ctx: OLLAMA_NUM_CTX,
          num_thread: OLLAMA_NUM_THREAD
        }
      }, { timeout: 60000 });

      this.isLoaded = true;
      console.log('‚úÖ Ollama model pre-warmed and ready');
      return true;
    } catch (error) {
      console.warn('‚ö†Ô∏è  Ollama pre-warm failed:', error.message);
      return false;
    }
  }

  /**
   * Load model explicitly into memory
   */
  async loadModel(modelName = OLLAMA_MODEL) {
    try {
      console.log(`üì• Loading model: ${modelName}...`);

      const response = await axios.post(`${OLLAMA_URL}/api/pull`, {
        name: modelName,
        stream: false
      }, { timeout: 300000 }); // 5 min timeout for download if needed

      this.modelName = modelName;
      this.isLoaded = true;
      console.log(`‚úÖ Model ${modelName} loaded`);
      return response.data;
    } catch (error) {
      console.error(`‚ùå Failed to load model ${modelName}:`, error.message);
      throw error;
    }
  }

  /**
   * Extract structured medical data from transcription text
   * @param {string} transcriptionText - Free-text transcription from Whisper
   * @param {string} language - Language code (ja, en, zh-TW)
   * @returns {Promise<object>} Structured medical data with confidence score
   */
  async extractStructuredData(transcriptionText, language = 'ja') {
    try {
      const systemPrompt = this.getSystemPrompt(language);
      const fullPrompt = `${systemPrompt}\n\nTranscription:\n${transcriptionText}\n\nExtracted Data (JSON only):`;

      console.log(`ü§ñ Extracting structured data (${language})...`);
      const startTime = Date.now();

      const response = await axios.post(`${OLLAMA_URL}/api/generate`, {
        model: this.modelName,
        prompt: fullPrompt,
        stream: false,
        format: 'json', // Request JSON output
        options: {
          temperature: OLLAMA_TEMPERATURE,
          num_ctx: OLLAMA_NUM_CTX,
          num_thread: OLLAMA_NUM_THREAD,
          num_gpu: 1, // Use Metal GPU on Mac
          top_p: 0.9,
          top_k: 40
        }
      }, {
        timeout: 120000, // 2 minutes max
        maxContentLength: 10 * 1024 * 1024 // 10MB max
      });

      const duration = ((Date.now() - startTime) / 1000).toFixed(2);
      console.log(`‚úÖ Extraction completed in ${duration}s`);

      // Parse response
      let extractedData;
      try {
        const responseText = response.data.response || response.data.text || '';
        extractedData = JSON.parse(responseText);
      } catch (parseError) {
        console.warn('‚ö†Ô∏è  JSON parse failed, attempting cleanup...');
        // Try to extract JSON from response
        const jsonMatch = response.data.response?.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          extractedData = JSON.parse(jsonMatch[0]);
        } else {
          throw new Error('Failed to extract valid JSON from LLM response');
        }
      }

      // Calculate confidence score
      const confidence = this.calculateConfidence(extractedData, transcriptionText);

      return {
        data: extractedData,
        confidence,
        processingTime: parseFloat(duration),
        model: this.modelName
      };

    } catch (error) {
      console.error('‚ùå Ollama extraction error:', error.message);
      throw error;
    }
  }

  /**
   * Generate chat completion (for future use)
   */
  async chat(messages, options = {}) {
    try {
      const response = await axios.post(`${OLLAMA_URL}/api/chat`, {
        model: this.modelName,
        messages: messages,
        stream: false,
        options: {
          temperature: options.temperature || OLLAMA_TEMPERATURE,
          num_ctx: options.num_ctx || OLLAMA_NUM_CTX,
          num_thread: OLLAMA_NUM_THREAD,
          num_gpu: 1
        }
      }, { timeout: 120000 });

      return response.data.message?.content || response.data.response;
    } catch (error) {
      console.error('‚ùå Ollama chat error:', error.message);
      throw error;
    }
  }

  /**
   * Unload model from memory
   */
  async unloadModel() {
    try {
      if (this.isLoaded) {
        console.log('üßπ Unloading Ollama model...');

        // Ollama doesn't have explicit unload, but we can clear context
        // The OS will swap out memory if needed
        this.isLoaded = false;

        // Suggest garbage collection
        if (global.gc) {
          global.gc();
        }

        console.log('‚úÖ Ollama model marked for unload');
      }
      return true;
    } catch (error) {
      console.warn('‚ö†Ô∏è  Ollama unload warning:', error.message);
      return false;
    }
  }

  /**
   * Get enhanced system prompt for Japanese nursing handoff documentation
   * VALIDATED PROMPT - Tested with YouTube nursing interaction data
   */
  getSystemPrompt(language) {
    const prompts = {
      'ja': `You are a medical translation and extraction AI. Extract structured data from Japanese medical transcripts and output ONLY valid JSON with ALL text translated to English.

CRITICAL RULES:
1. ALL Japanese text MUST be translated to English
2. NO Japanese characters (hiragana, katakana, kanji) in output
3. Output ONLY valid JSON - no markdown, no explanations
4. If unsure about translation, provide best English approximation

REQUIRED JSON SCHEMA:
{
  "patients": [{ "room": "string or null", "name": "string or null", "status": "string or null" }],
  "vital_signs": [{ "patient": "string", "time": "HH:MM", "temperature": "NN.N C", "oxygen_saturation": "NN %", "oxygen_flow": "N L/min" }],
  "observations": ["English observation 1", "English observation 2"],
  "actions_taken": ["English action 1", "English action 2"],
  "follow_up_needed": ["English follow-up 1"]
}

TRANSLATION EXAMPLES:
Japanese ‚Üí English
- "Âí≥„ÄÅÈºªÊ∞¥" ‚Üí "Cough and runny nose"
- "ÂæÆÁÜ±" ‚Üí "Mild fever"
- "„Ç≥„É≠„Éä„Å®„Ç§„É≥„Éï„É´„Ç®„É≥„Ç∂„ÅÆÊ§úÊüª„ÅØÈô∞ÊÄß" ‚Üí "COVID and influenza tests negative"
- "„ÅäËÉ∏„ÅÆÈü≥„ÇíËÅû„Åç„Åæ„Åó„Åü" ‚Üí "Listened to chest sounds"
- "ÁóáÁä∂„Å´ÂØæ„Åô„Çã„ÅäËñ¨„Çí„ÅäÂá∫„Åó„Åó„Åæ„Åó„Åü" ‚Üí "Prescribed medication for symptoms"
- "‰øùËÇ≤Âúí„ÇÑÂπºÁ®öÂúí„Å´ÈÄöË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ" ‚Üí "May return to daycare/kindergarten when symptoms improve"
- "36Â∫¶8ÂàÜ" ‚Üí "36.8 C"
- "„Çµ„ÉÅ„É•97%" ‚Üí "97% SpO2"
- "ÈÖ∏Á¥†3„É™„ÉÉ„Éà„É´" ‚Üí "3 L/min oxygen"

MEDICAL TERM TRANSLATIONS:
- ÁÇπÊª¥ ‚Üí IV drip
- Ëß£ÁÜ± ‚Üí Antipyretic/fever reduction
- ÈÖ∏Á¥† ‚Üí Oxygen
- ‰ΩìÊ∏© ‚Üí Body temperature
- Ë°ÄÂúß ‚Üí Blood pressure
- ËÑàÊãç ‚Üí Pulse
- ÂëºÂê∏ ‚Üí Respiration
- ÊÑèË≠ò ‚Üí Consciousness
- È£ü‰∫ã ‚Üí Meal intake
- ÊéíÊ≥Ñ ‚Üí Bowel/bladder output

Remember: The user cannot read Japanese. Every single word must be in English.`,

      'en': `SYSTEM ROLE:
You are a clinical documentation AI. You must ALWAYS output valid JSON and nothing else.

TASK:
Extract structured data from nursing documentation transcripts.
- Fill all required keys; if unknown, use null (not "").
- Do not invent information not present.

SCHEMA (must match exactly):
{
  "patients": [{ "room": "", "name": "", "status": "" }],
  "vital_signs": [{ "patient": "", "time": "", "temperature": "", "oxygen_saturation": "", "oxygen_flow": "" }],
  "observations": [""],
  "actions_taken": [""],
  "follow_up_needed": [""]
}

NORMALIZATION RULES:
- Rooms: keep as strings (e.g., "502", "ICU-3")
- Times: "HH:MM" 24h format
- Temperature: "NN.N C" (valid range 30-42)
- SpO2: "NN %" or "NN‚ÄìNN %"
- Oxygen flow: "N L/min" or "N‚ÄìN L/min"

GUARDRAILS:
- Always return valid JSON that can be parsed.
- Arrays must exist even if empty.
- Deduplicate repeated or similar items.`,

      'zh-TW': `SYSTEM ROLE:
‰Ω†ÊòØËá®Â∫äÊñá‰ª∂AI„ÄÇ‰Ω†ÂøÖÈ†àÂßãÁµÇËº∏Âá∫ÊúâÊïàÁöÑJSONÔºå‰∏çËÉΩÊúâÂÖ∂‰ªñÂÖßÂÆπ„ÄÇ

‰ªªÂãô:
ÂæûË≠∑ÁêÜ‰∫§Êé•Áè≠Ë®òÈåÑ‰∏≠ÊèêÂèñÁµêÊßãÂåñÊï∏Êìö„ÄÇ
- Â∞áÊâÄÊúâÂÖßÂÆπÁøªË≠ØÊàêËã±Êñá„ÄÇ
- Âö¥Ê†ºÈÅµÂÆàschema„ÄÇ
- Â°´ÂØ´ÊâÄÊúâÂøÖÂ°´Â≠óÊÆµÔºõÂ¶ÇÊûúÊú™Áü•Ôºå‰ΩøÁî®nullÔºà‰∏çÊòØ""Ôºâ„ÄÇ
- ‰∏çË¶ÅÁ∑®ÈÄ†‰ø°ÊÅØ„ÄÇ

SCHEMA (ÂøÖÈ†àÂÆåÂÖ®ÂåπÈÖç):
{
  "patients": [{ "room": "", "name": "", "status": "" }],
  "vital_signs": [{ "patient": "", "time": "", "temperature": "", "oxygen_saturation": "", "oxygen_flow": "" }],
  "observations": [""],
  "actions_taken": [""],
  "follow_up_needed": [""]
}

Ë¶èÁØÑÂåñË¶èÂâá:
- ÊàøÈñì: ‰øùÊåÅÊï∏Â≠óÂ≠óÁ¨¶‰∏≤Â¶Ç"502"
- ÊôÇÈñì: "HH:MM" 24Â∞èÊôÇÂà∂
- È´îÊ∫´: "NN.N C" (ÊúâÊïàÁØÑÂúç30-42)
- SpO2: "NN %" Êàñ "NN‚ÄìNN %"
- Ê∞ßÊ∞£ÊµÅÈáè: "N L/min" Êàñ "N‚ÄìN L/min"

‰øùË≠∑Êé™ÊñΩ:
- ÂßãÁµÇËøîÂõûÂèØËß£ÊûêÁöÑÊúâÊïàJSON„ÄÇ
- Êï∏ÁµÑÂøÖÈ†àÂ≠òÂú®ÔºåÂç≥‰ΩøÁÇ∫Á©∫„ÄÇ
- ÂéªÈáçË§áÊàñÁõ∏‰ººÈ†ÖÁõÆ„ÄÇ`
    };

    return prompts[language] || prompts['en'];
  }

  /**
   * Calculate confidence score based on data completeness and validity
   */
  calculateConfidence(extractedData, originalText) {
    let score = 0.5; // Base score

    // Check data completeness
    let filledFields = 0;
    let totalFields = 0;

    const countFields = (obj) => {
      for (const value of Object.values(obj)) {
        totalFields++;
        if (value !== null && value !== undefined) {
          if (typeof value === 'object' && !Array.isArray(value)) {
            countFields(value);
          } else {
            filledFields++;
          }
        }
      }
    };

    if (extractedData) {
      countFields(extractedData);
      score = filledFields / Math.max(totalFields, 1);
    }

    // Boost confidence if text length suggests detailed documentation
    if (originalText && originalText.length > 100) {
      score = Math.min(0.95, score + 0.1);
    }

    // Cap confidence at 92% for local model
    return Math.min(0.92, Math.max(0.6, score));
  }

  /**
   * Get service health status
   */
  async getHealth() {
    try {
      const response = await axios.get(`${OLLAMA_URL}/api/tags`, { timeout: 3000 });
      const models = response.data.models || [];

      return {
        available: true,
        model: this.modelName,
        url: OLLAMA_URL,
        loaded: this.isLoaded,
        availableModels: models.map(m => m.name),
        config: {
          num_ctx: OLLAMA_NUM_CTX,
          num_thread: OLLAMA_NUM_THREAD,
          temperature: OLLAMA_TEMPERATURE
        }
      };
    } catch (error) {
      return {
        available: false,
        model: this.modelName,
        url: OLLAMA_URL,
        loaded: false,
        error: error.message
      };
    }
  }
}

// Singleton instance
const ollamaService = new OllamaService();

// Export both the class and singleton
export { OllamaService };
export default ollamaService;